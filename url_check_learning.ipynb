{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入所需库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import reshape\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df1 = pd.read_csv('Webpages_Classification_train_data.csv').drop(['Unnamed: 0'], axis=1)\n",
    "df2 = pd.read_csv('Webpages_Classification_test_data.csv').drop(['Unnamed: 0'], axis=1)\n",
    "    \n",
    "concatenated_df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>url_len</th>\n",
       "      <th>ip_add</th>\n",
       "      <th>geo_loc</th>\n",
       "      <th>tld</th>\n",
       "      <th>who_is</th>\n",
       "      <th>https</th>\n",
       "      <th>js_len</th>\n",
       "      <th>js_obf_len</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://members.tripod.com/russiastation/</td>\n",
       "      <td>40</td>\n",
       "      <td>42.77.221.155</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>com</td>\n",
       "      <td>complete</td>\n",
       "      <td>yes</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Named themselves charged particles in a manly ...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.ddj.com/cpp/184403822</td>\n",
       "      <td>32</td>\n",
       "      <td>3.211.202.180</td>\n",
       "      <td>United States</td>\n",
       "      <td>com</td>\n",
       "      <td>complete</td>\n",
       "      <td>yes</td>\n",
       "      <td>52.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>And filipino field \\n \\n \\n \\n \\n \\n \\n \\n the...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.naef-usa.com/</td>\n",
       "      <td>24</td>\n",
       "      <td>24.232.54.41</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>com</td>\n",
       "      <td>complete</td>\n",
       "      <td>yes</td>\n",
       "      <td>103.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Took in cognitivism, whose adherents argue for...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.ff-b2b.de/</td>\n",
       "      <td>21</td>\n",
       "      <td>147.22.38.45</td>\n",
       "      <td>United States</td>\n",
       "      <td>de</td>\n",
       "      <td>incomplete</td>\n",
       "      <td>no</td>\n",
       "      <td>720.0</td>\n",
       "      <td>532.8</td>\n",
       "      <td>fire cumshot sodomize footaction tortur failed...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://us.imdb.com/title/tt0176269/</td>\n",
       "      <td>35</td>\n",
       "      <td>205.30.239.85</td>\n",
       "      <td>United States</td>\n",
       "      <td>com</td>\n",
       "      <td>complete</td>\n",
       "      <td>yes</td>\n",
       "      <td>46.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Levant, also monsignor georges. In 1800, lists...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        url  url_len         ip_add  \\\n",
       "0  http://members.tripod.com/russiastation/       40  42.77.221.155   \n",
       "1          http://www.ddj.com/cpp/184403822       32  3.211.202.180   \n",
       "2                  http://www.naef-usa.com/       24   24.232.54.41   \n",
       "3                     http://www.ff-b2b.de/       21   147.22.38.45   \n",
       "4       http://us.imdb.com/title/tt0176269/       35  205.30.239.85   \n",
       "\n",
       "         geo_loc  tld      who_is https  js_len  js_obf_len  \\\n",
       "0         Taiwan  com    complete   yes    58.0         0.0   \n",
       "1  United States  com    complete   yes    52.5         0.0   \n",
       "2      Argentina  com    complete   yes   103.5         0.0   \n",
       "3  United States   de  incomplete    no   720.0       532.8   \n",
       "4  United States  com    complete   yes    46.5         0.0   \n",
       "\n",
       "                                             content label  \n",
       "0  Named themselves charged particles in a manly ...  good  \n",
       "1  And filipino field \\n \\n \\n \\n \\n \\n \\n \\n the...  good  \n",
       "2  Took in cognitivism, whose adherents argue for...  good  \n",
       "3  fire cumshot sodomize footaction tortur failed...   bad  \n",
       "4  Levant, also monsignor georges. In 1800, lists...  good  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.label.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.label.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an equally distributed sample\n",
    "concatenated_df_good_equally_bad = concatenated_df.groupby('label').apply(lambda x: x.sample(30000, random_state=42)).reset_index(drop=True)\n",
    "# Remove if content has less than 60 words\n",
    "concatenated_df_good_equally_bad = concatenated_df_good_equally_bad[concatenated_df_good_equally_bad.content.str.split().str.len().ge(60)]\n",
    "concatenated_df_good_equally_bad.label.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df_good_equally_bad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample trimmed dataframe to make it uniformly distributed\n",
    "re_concatenated_df_good_equally_bad = concatenated_df_good_equally_bad.groupby('label').apply(lambda x: x.sample(3000, random_state=42)).reset_index(drop=True)\n",
    "# Randomly shuffle rows for aesthetics\n",
    "re_concatenated_df_good_equally_bad = re_concatenated_df_good_equally_bad.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "re_concatenated_df_good_equally_bad.label.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_concatenated_df_good_equally_bad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_concatenated_df_good_equally_bad[['geo_loc', 'tld','who_is','https', 'label']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_concatenated_df_good_equally_bad['geo_loc'] = OrdinalEncoder().fit_transform(re_concatenated_df_good_equally_bad.geo_loc.values.reshape(-1,1))\n",
    "re_concatenated_df_good_equally_bad['tld'] = OrdinalEncoder().fit_transform(re_concatenated_df_good_equally_bad.tld.values.reshape(-1,1))\n",
    "re_concatenated_df_good_equally_bad['who_is'] = OrdinalEncoder().fit_transform(re_concatenated_df_good_equally_bad.who_is.values.reshape(-1,1))\n",
    "re_concatenated_df_good_equally_bad['https'] = OrdinalEncoder().fit_transform(re_concatenated_df_good_equally_bad.https.values.reshape(-1,1))\n",
    "re_concatenated_df_good_equally_bad['label'] = OrdinalEncoder().fit_transform(re_concatenated_df_good_equally_bad.label.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_concatenated_df_good_equally_bad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert url into human readable string that can be tokenized\n",
    "re_concatenated_df_good_equally_bad['url'] = re_concatenated_df_good_equally_bad.url.apply(lambda x: ' '.join(x.split('://')[1].strip('www.').replace('.','/').split('/')))\n",
    "re_concatenated_df_good_equally_bad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Before Preprocessing:\")\n",
    "print(re_concatenated_df_good_equally_bad.content.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tqdm.pandas()\n",
    "stop = stopwords.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re_concatenated_df_good_equally_bad.content = re_concatenated_df_good_equally_bad.content.str.replace(\"[^\\w\\s]\", \"\").str.lower()\n",
    "re_concatenated_df_good_equally_bad.content = re_concatenated_df_good_equally_bad.content.progress_apply(lambda x: ' '.join([item for item in x.split() \n",
    "                                                               if item not in stop]))\n",
    "re_concatenated_df_good_equally_bad.url = re_concatenated_df_good_equally_bad.url.str.replace(\"[^\\w\\s]\", \"\").str.lower()\n",
    "re_concatenated_df_good_equally_bad.url = re_concatenated_df_good_equally_bad.url.progress_apply(lambda x: ' '.join([item for item in x.split() \n",
    "                                                               if item not in stop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"After Preprocessing:\")\n",
    "print(re_concatenated_df_good_equally_bad.content.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    min_df = 5,\n",
    "    max_df = 0.95,\n",
    "    max_features = 8000,\n",
    "    stop_words = 'english'\n",
    ")\n",
    "\n",
    "tfidf.fit(re_concatenated_df_good_equally_bad.url)\n",
    "url_tfidf = tfidf.transform(re_concatenated_df_good_equally_bad.url)\n",
    "\n",
    "tfidf.fit(re_concatenated_df_good_equally_bad.content)\n",
    "content_tfidf = tfidf.transform(re_concatenated_df_good_equally_bad.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re_concatenated_df_good_equally_bad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def find_optimal_clusters(data, max_k):\n",
    "    k_list = range(2, max_k+1)\n",
    "    \n",
    "    sse = []\n",
    "    for k in k_list:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n",
    "       \n",
    "    plt.style.use(\"dark_background\")\n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(k_list, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centers')\n",
    "    ax.set_xticks(k_list)\n",
    "    ax.set_xticklabels(k_list)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE by Cluster Center Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find_optimal_clusters(url_tfidf, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re_concatenated_df_good_equally_bad['url_cluster'] = MiniBatchKMeans(n_clusters=9, init_size=1024, batch_size=2048, \n",
    "                                            random_state=20).fit_predict(url_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find_optimal_clusters(content_tfidf, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re_concatenated_df_good_equally_bad['content_cluster'] = MiniBatchKMeans(n_clusters=5, init_size=1024, batch_size=2048, \n",
    "                                            random_state=20).fit_predict(content_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re_concatenated_df_good_equally_bad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentenceTransformer_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re_concatenated_df_good_equally_bad['url'] = SentenceTransformer_model.encode(re_concatenated_df_good_equally_bad['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_concatenated_df_good_equally_bad['url'] = re_concatenated_df_good_equally_bad.url.apply(lambda x:SentenceTransformer_model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re_concatenated_df_good_equally_bad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re_concatenated_df_good_equally_bad['content'] = SentenceTransformer_model.encode(re_concatenated_df_good_equally_bad['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_concatenated_df_good_equally_bad['content'] = re_concatenated_df_good_equally_bad.content.apply(lambda x:SentenceTransformer_model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_concatenated_df_good_equally_bad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_concatenated_df_good_equally_bad.url.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分离特征和标签\n",
    "\n",
    "X = re_concatenated_df_good_equally_bad.url\n",
    "#X = re_concatenated_df_good_equally_bad[['url_cluster', 'url_len', 'geo_loc', 'tld', 'who_is', 'https', 'content_cluster',\n",
    "                #'js_len', 'js_obf_len']]\n",
    "y = re_concatenated_df_good_equally_bad.label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用LogisticRegression模型\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "params = {\n",
    "    'C': [0.1, 0.5, 1.0],\n",
    "    'solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "grid = GridSearchCV(model, params, cv=5)\n",
    "\n",
    "\n",
    "grid.fit([i for i in X_train], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters for LogisticRegression: \", grid.best_params_)\n",
    "print(\"Best score for LogisticRegression: \", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用LogisticRegression进行训练\n",
    "best_C = 1.0\n",
    "best_solver = 'lbfgs'\n",
    "model = LogisticRegression(C=best_C, solver=best_solver)\n",
    "model.fit([i for i in X_train], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict([i for i in X_train])\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Accuracy: \", accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict([i for i in X_test])\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy: \", accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_check(url):\n",
    "    embedding = SentenceTransformer_model.encode(url)\n",
    "    embedding = embedding.reshape(1,-1)\n",
    "    prediction = model.predict(embedding)\n",
    "    dic = { 1:'good', 0:'bad'}\n",
    "    print(\"Prediction for new URL: \", dic[prediction[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_check(\"http://www.cheernudes.com/lesbian/1024lss/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_check(\"http://www.example.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# Create your model here (same as above)\n",
    "#\n",
    "# Save to file in the current working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "pkl_filename = \"url_check_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "pkl_filename = \"url_check_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = pickle_model.predict([i for i in X_train])\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Accuracy: \", accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = pickle_model.predict([i for i in X_test])\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy: \", accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
